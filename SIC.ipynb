{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1TVwLBqum7zTHqHLSZijprFJ7pvby6_JC",
      "authorship_tag": "ABX9TyNCo79hUNM/fK39rNqP5Zex",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ToAN7/SIC_AI/blob/TrongKha_/SIC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "PiXh1xeoTm4q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03e32a0e-bf48-41e8-9e80-3096ab9db736"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "id": "jWmVM1gPbAsM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6ba0f82-d403-45d8-fd51-9aa76b6dc2cd"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.42.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.3.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.23.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence_transformers) (12.6.20)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "id": "PuFN2yRKDXws"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#import thư viện"
      ],
      "metadata": {
        "id": "ZbnwSmFla7Sc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVR# mô hình chạy thử\n",
        "from sklearn.svm import SVC# mô hình chạy thử\n",
        "from sklearn.neighbors import KNeighborsClassifier# mô hình chạy thử\n",
        "import pandas as pd# lấy dữ liệu\n",
        "import numpy as np # tính toán\n",
        "import matplotlib.pyplot as plt # vẽ đồ thị\n",
        "from sklearn.model_selection import train_test_split # Chia dữ liệu\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix #đánh giá mô hình\n",
        "import re # tiền xử lý\n",
        "import string # tiền xử lý\n",
        "from sklearn.decomposition import PCA # giảm chiều dữ liệu\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer # Tính tfidf\n",
        "import joblib"
      ],
      "metadata": {
        "id": "6sjZr1aqa7GV"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Hàm"
      ],
      "metadata": {
        "id": "-chsJnlkaow9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vietnamese_stop_words = [\n",
        "              'là', 'và', 'có', 'trên', 'cho', 'một', 'các', 'được', 'như', 'với',\n",
        "              'của', 'ở', 'khi', 'đã', 'còn', 'thì', 'này', 'bởi', 'đó', 'để', 'năm',\n",
        "              'ngày', 'vào', 'sau', 'tại', 'trong', 'rằng', 'đến', 'từ'\n",
        "              ]"
      ],
      "metadata": {
        "id": "jVOuwTx6a2eL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer('/content/drive/MyDrive/SIC/model_doc2vec')"
      ],
      "metadata": {
        "id": "JP3sCzu4a3dt"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Hàm làm sạch dữ liệu <nên dùng trước khi vector hoá dữ liệu>\n",
        "def convertData(text):\n",
        "  text = text.lower()\n",
        "  text = re.sub('\\[.*?\\]','',text)\n",
        "  text = re.sub(\"\\\\W\",\" \",text)\n",
        "  text = re.sub('https?://\\S+|www\\.\\S+','',text)\n",
        "  text = re.sub('<.*?>+',b'',text)\n",
        "  text = re.sub('[%s]' % re.escape(string.punctuation),'',text)\n",
        "  text = re.sub('\\w*\\d\\w*','',text)\n",
        "  text = re.sub(' +', ' ',text)\n",
        "  text = text.strip()\n",
        "  text = ' '.join([word for word in text.split() if word not in vietnamese_stop_words])\n",
        "  return text"
      ],
      "metadata": {
        "id": "ZqLPFH-4azWD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Cosine_similarity"
      ],
      "metadata": {
        "id": "2IydxwjDsmK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Hàm để tính Cosine\n",
        "def cosine_similarity(x_vec,y_vec, flag):\n",
        "  cosine_similarities = x_vec.dot(y_vec.T)\n",
        "  if flag == True:# Trường hợp trả kết quả với dữ liệu tfIdf\n",
        "    return cosine_similarities.toarray().squeeze()#\n",
        "  else:# Trường hợp trả kết quả với dữ liệu ebd\n",
        "    ll = []\n",
        "    for i in range(cosine_similarities.shape[0]):\n",
        "      ll.append(cosine_similarities[i].max())\n",
        "    return ll"
      ],
      "metadata": {
        "id": "Y2Q2w-DAar8l"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Hàm kiểm thử cho đánh giá độ tương đồng Cosine\n",
        "def Cosine_sml(fake_news, len_real,real_vectors):\n",
        "  # lấy chỉ số của từng fake_news\n",
        "  content = [convertData(txt) for txt in fake_news['Fake_Content']]# làm sạch từng bài\n",
        "  vectors_content = vectorizer.transform(content)\n",
        "  cosine_similarities = cosine_similarity(vectors_content,real_vectors, True) # gọi hàm để tính Cosine\n",
        "  danhgia_f = 0\n",
        "  for idx, x in enumerate(cosine_similarities):\n",
        "    if x.max() < 0.5:\n",
        "      danhgia_f +=1\n",
        "  print(f'Tổng số tin giả của fake_news : {len(fake_news)}')\n",
        "  print(f'Số lượng tin được phát hiện có khả năng là giả của fake_news: {danhgia_f}')\n",
        "  print(f\"Độ chính xác khi dự đoán tin giả : {danhgia_f/len(fake_news)}\")\n",
        "  print('_____________________________________________________________________________________')\n",
        "  return danhgia_f/len(fake_news)"
      ],
      "metadata": {
        "id": "x3E0L7_Fpvjs"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Hàm kiểm thử cho đánh giá độ tương đồng Cosine\n",
        "def Cosine_sml_ebd(fake_vectors,real_vectors):#fake_vectors va real_vectors\n",
        "  cosine_similarities = cosine_similarity(fake_vectors,real_vectors, False)\n",
        "  danhgia_f = 0\n",
        "  for idx, x in enumerate(cosine_similarities):\n",
        "    if x.max() < 0.5:\n",
        "      danhgia_f +=1\n",
        "  print(f'Tổng số tin giả của fake_ : {len(fake_vectors)}')\n",
        "  print(f'Số lượng tin được phát hiện có khả năng là giả của fake_: {danhgia_f}')\n",
        "  print(f\"Độ chính xác khi dự đoán tin giả : {danhgia_f/len(fake_vectors)}\")\n",
        "  print('_____________________________________________________________________________________')\n",
        "  return danhgia_f/len(fake_vectors)"
      ],
      "metadata": {
        "id": "xz6co93Gnko_"
      },
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Gọi dữ liệu test"
      ],
      "metadata": {
        "id": "wbI66v_neSve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#real_news là phần tập dữ liệu gốc (8943 bài viết-tin thật) chưa convert và chưa vector hoá\n",
        "# 2 tập này chưa xử lý gì hết\n",
        "real_news = pd.read_csv('/content/drive/MyDrive/Data_/Data_test_300/DATA_.csv')#Tập tin có thể là thật\n",
        "teptingia = pd.read_csv('/content/drive/MyDrive/Data_/dulieutin_gia_4000.csv')#Tập tin có thể là giả\n",
        "#2 tập ebd này đã được dùng bert để vector hoá\n",
        "ebd_teptingia = pd.read_csv('/content/drive/MyDrive/Data_/Fake_Ebd_4000.csv') # 4000 dữ liệu đầu tiên\n",
        "ebd_teptinthat = pd.read_csv('/content/drive/MyDrive/SIC/Embeddings_DATA.csv')\n",
        "real_news.drop(columns=['Unnamed: 0'],inplace=True)\n",
        "# real_news.columns = ['Content']\n",
        "\n",
        "real_news = real_news[4000:]\n",
        "real_news.reset_index(inplace=True)\n",
        "real_news.drop(columns=['index'],inplace=True)\n",
        "teptingia.drop(columns=['Unnamed: 0'],inplace=True)\n",
        "teptingia.reset_index(inplace=True)\n",
        "teptingia.drop(columns=['Unnamed: 0.1'],inplace=True)\n",
        "teptingia.drop(columns=['index'],inplace=True)\n",
        "ebd_teptinthat.drop(columns=['Unnamed: 0'],inplace=True)\n",
        "ebd_teptingia.drop(columns=['Unnamed: 0'],inplace=True)\n",
        "ebd_teptinthat = ebd_teptinthat[4000:]\n",
        "ebd_teptinthat.reset_index(inplace=True)\n",
        "ebd_teptinthat.drop(columns=['index'],inplace=True)"
      ],
      "metadata": {
        "id": "lZrNj4xseSH0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c674e90-0e92-4fcd-b423-f26cf3380427"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-fd2884e2c613>:13: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  real_news.drop(columns=['index'],inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ketqua_tfidf = list()\n",
        "ketqua_ebd = list()"
      ],
      "metadata": {
        "id": "BGyanmEjdhSC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Độ tương đồng Cosine"
      ],
      "metadata": {
        "id": "ICP1z9Hrwr2l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>TfIDF</h1>"
      ],
      "metadata": {
        "id": "2sIz-4WgxwPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "real_vectors = vectorizer.fit_transform(real_news['Content'])"
      ],
      "metadata": {
        "id": "LxQEArq6UTWG"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ketqua_tfidf.append(Cosine_sml(teptingia,[len(real_news)],real_vectors))#tfidf"
      ],
      "metadata": {
        "id": "yGTYknZbkKUh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95bb1e2a-493b-4139-a184-2c29c1429810"
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tổng số tin giả của fake_news : 4000\n",
            "Số lượng tin được phát hiện có khả năng là giả của fake_news: 2170\n",
            "Độ chính xác khi dự đoán tin giả : 0.5425\n",
            "_____________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Bert</h1>"
      ],
      "metadata": {
        "id": "1xUkK3N3xp2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ketqua_ebd.append(Cosine_sml_ebd(ebd_teptingia,ebd_teptinthat))"
      ],
      "metadata": {
        "id": "U6WaA48p5HmO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "088addc8-fa46-46be-de69-8bfa89054555"
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tổng số tin giả của fake_ : 4000\n",
            "Số lượng tin được phát hiện có khả năng là giả của fake_: 0\n",
            "Độ chính xác khi dự đoán tin giả : 0.0\n",
            "_____________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SVC - KNN"
      ],
      "metadata": {
        "id": "Zz75VbNjgtva"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sử dụng tfIdf để vector hoá"
      ],
      "metadata": {
        "id": "EumKuvJtfB4u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM_SVC"
      ],
      "metadata": {
        "id": "Q3sRb5AuVWV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#gọi model\n",
        "svc_model = joblib.load('/content/drive/MyDrive/SIC/svc_model_update.pkl')"
      ],
      "metadata": {
        "id": "qIF2QPMl63Lb"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Làm sạch\n",
        "X_Fake_tfidf = [convertData(teptingia['Fake_Content'][i]) for i in range(len(teptingia['Fake_Content']))]\n",
        "X_Real_tfidf = [convertData(real_news['Content'][i]) for i in range(len(real_news['Content']))]\n",
        "# Tổng hợp và đánh nhãn\n",
        "X = X_Real_tfidf + X_Fake_tfidf\n",
        "Y = [1]*len(X_Real_tfidf) + [0]*len(X_Fake_tfidf)\n",
        "#Chia dữ liêu train và test : 80 20\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "XbmfoEuJl_ID"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = joblib.load('/content/drive/MyDrive/SIC/vectorizer.joblib')\n",
        "X_train_tfidf = vectorizer.transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)"
      ],
      "metadata": {
        "id": "irZnXa6anQst"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = svc_model.predict(X_test_tfidf)\n",
        "#kiểm tra X_test_tfidf\n",
        "# Calculate the accuracy of the model.\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "ketqua_tfidf.append(accuracy)\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
        "# Display the classification report and confusion matrix.\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "id": "KgKit75YoBdE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efa67efe-acb5-41b8-aa0d-ba6c37f8a4e9"
      },
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 78.42%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.79      0.77       819\n",
            "           1       0.82      0.78      0.80       970\n",
            "\n",
            "    accuracy                           0.78      1789\n",
            "   macro avg       0.78      0.79      0.78      1789\n",
            "weighted avg       0.79      0.78      0.78      1789\n",
            "\n",
            "Confusion Matrix:\n",
            "[[651 168]\n",
            " [218 752]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "KNN"
      ],
      "metadata": {
        "id": "xPok29YuuZsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "from joblib import load as ld\n",
        "knn_model = ld('/content/drive/MyDrive/SIC/knn_model_Update.joblib')"
      ],
      "metadata": {
        "id": "2N_OAXfvua7h"
      },
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = knn_model.predict(X_test_tfidf)\n",
        "# Calculate the accuracy of the model.\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "ketqua_tfidf.append(accuracy)\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
        "# Display the classification report and confusion matrix.\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "#bỏ KMeans"
      ],
      "metadata": {
        "id": "3FsTgfAcupTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sử dụng bert để vector hoá"
      ],
      "metadata": {
        "id": "y20GStMGfJMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svc_model_ebd = joblib.load('/content/drive/MyDrive/SIC/svc_model_ebd.pkl')"
      ],
      "metadata": {
        "id": "zqbWaSp6DiXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_bert = pd.concat([ebd_teptinthat,ebd_teptingia])\n",
        "y_bert = [1]*len(ebd_teptinthat) + [0]*len(ebd_teptingia)\n",
        "X_bert.reset_index(inplace=True)\n",
        "X_bert.drop(columns=['index'],inplace=True)"
      ],
      "metadata": {
        "id": "Yaka-PwzO4vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_b, X_test_b, y_train_b, y_test_b = train_test_split(X_bert, y_bert, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "3rd0_dJ42r1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = svc_model_ebd.predict(X_test_b)\n",
        "#kiểm tra X_test_tfidf\n",
        "# Calculate the accuracy of the model.\n",
        "accuracy = accuracy_score(y_test_b, y_pred)\n",
        "ketqua_ebd.append(accuracy)\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
        "# Display the classification report and confusion matrix.\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test_b, y_pred))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test_b, y_pred))"
      ],
      "metadata": {
        "id": "ipAGIG9V3SP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "knn_model_ebd = joblib.load('/content/drive/MyDrive/SIC/knn_model_ebd.pkl')"
      ],
      "metadata": {
        "id": "txJyeRYQFf7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = knn_model_ebd.predict(X_test_b)\n",
        "#kiểm tra X_test_tfidf\n",
        "# Calculate the accuracy of the model.\n",
        "accuracy = accuracy_score(y_test_b, y_pred)\n",
        "ketqua_ebd.append(accuracy)\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
        "# Display the classification report and confusion matrix.\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test_b, y_pred))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test_b, y_pred))"
      ],
      "metadata": {
        "id": "nq4ylvs4AIm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Vẽ kết quả"
      ],
      "metadata": {
        "id": "fUsXzrxpyDos"
      }
    }
  ]
}
